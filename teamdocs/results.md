## Overview:
For our project, we decided to look at an Amazon Dataset that showed which items were commonly bought with other items. The four main parts of the project (parsing the data, creating our DFS traversal, implementing the PageRank algorithm, and implementing the Betweenness Centrality algorithm) were able to be met with some slight optimization/adjustments to our original approach. We were able to confirm that our results were true by writing extensive tests for each of this project. Our tests were essentially just smaller data sets that represented possible complications we might occur: directed versus undirected or connected versus unconnected. 
## DFS:
Our DFS implementation currently outputs the DFS traversal through the graph into a file (deliverables/Amazon_Items_Traversal.txt). We begin at node 0, and do a depth first search from there. In addition, we implemented an iterator that allows us to iterate through the items of the DFS traversal more easily/efficiently and also track where items are in the traversal. Our DFS class also counts the number of connected components within the graph to allow for better data analysis.
## PageRank:
PageRank was one of our main algorithms that we needed to implement. Traditionally, the PageRank algorithm attempts to find the long-term probability that a certain link will be reached. So, the probabilities that each link will be reached is constantly multiplied by the Google Page Rank Matrix. In our case, we are seeing the odds that a certain Amazon product will have been purchased in conjunction with other products. 

Our PageRank Algorithm is running a maximum of 10 iterations to find the long-term stable state. We could have done more iterations, but we saw that the data did not trend to be that different with more iterations, so it was not worth the additional time cost. Additionally, we took advantage of our PageRank Matrix mainly containing the same values by making it sparse. This way we were able to optimize our algorithm to run in O(E) time compared to O(n^2) time.

Our final deliverable for this algorithm is a file (Amazon_Items_PageRank.txt) that lists the nodes in order of most important to least important. With our most important node being node 8, and our least important being node 5018. 

These results agree with our assumptions that the items that are commonly purchased with other items (i.e. having many edges pointing to itself) are typically the more important nodes. For reference 'node 8' has 293 incoming edges on the 1234877 edge data set, while 'node 5018' only has 2 incoming edges. 
## Betweenness Centrality:
Betweenness Centrality was a key component of our analysis in order to determine which nodes are most central when traversing through the shortest paths in the graph. This allows us to analyze the probability of traversing through a specific node on the shortest path to another node. Similarly to PageRank, we can see a number of Amazon products that are “hub nodes” where many traversals travel through them. 

Our Betweenness Centrality is running a more optimized Brandes algorithm (Link to the referenced paper here). This allows our algorithm to have a more optimized O(V * E) as opposed O(n^3). The current algorithm does a BFS to find the shortest path on the directed graph and then takes the distance and shortest path predecessors to figure out the betweenness centrality scores. Our current scores are not normalized which means that they just represent the number of times that a certain node has been passed in a shortest path traversal.

The current tests prove our assumption that the Betweenness Centrality values are calculated correctly since we independently calculated them against the current code output. In addition, the output ranks the nodes by Betweenness Centrality value, so we can see that there are central nodes that are needed to traverse between parts of the graph. 

The output also proves our assumption that certain nodes will be central to traverse through in a shortest path traversal between nodes in a directed graph of Amazon products.
## Discussion:
The most interesting part of our data analysis is when we compare the node ranks of Betweenness Centrality and PageRank. We did additional calculations that showed that the difference in node rank for pagerank versus betweenness centrality was consistently ~25% across our data sets. This  suggests that the flow of products and their centrality in traversal is somewhat correlated. However, this number doesn’t show the full story. For example, node 8 is very close in both ranks (1 for PageRank and 5 for Betweenness Centrality), but node 9 has a huge difference in rank (24 for PageRank and 9989 for Betweenness Centrality). It shows an average difference and therefore gives us a high-level difference in the dataset generally, but it is also likely that large outliers make the difference larger than it seems. One thing to note is that endpoints in particular will have a large disparity in their PageRank and Betweenness Centrality since there are no outgoing edges and therefore the Betweenness Centrality is 0, but for PageRank a large number of products flow into the endpoint since there is no outgoing flow.
